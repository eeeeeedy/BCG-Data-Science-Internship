{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/eeeeeedy/BGC/blob/main/assets/BCG_MONOGRAM.png?raw=true\" alt = \"BCG  icon\" width=\"20%\" height=\"20%\">\n",
    "</p>\n",
    "\n",
    "# **Feature Engineering & Modelling**\n",
    "By [Edy Setiawan](https://github.com/eeeeeedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working on two main sub-tasks:\n",
    "\n",
    "    Sub-Task 1: Focus on feature engineering, specifically on improving a feature calculated as \"the difference between off-peak prices in December and January the preceding year.\"\n",
    "    \n",
    "    Sub-Task 2: Build a predictive model using Random Forest to predict customer churn. This involves evaluating the model rigorously, discussing the pros and cons of using a Random Forest for this use-case, and optionally tying model performance to the client's financial metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries for Feature Engineering and Modeling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Task 1: Feature Engineering\n",
    "\n",
    "We'll start by recreating the feature your colleague has worked on. Then, we'll think of ways to improve its predictive power.\n",
    "\n",
    "#### Steps:\n",
    "1. Load the cleaned data (`client_df`) and pricing data (`price_df`).\n",
    "2. Create the feature as per your colleague's notebook.\n",
    "3. Enhance the feature's predictive power.\n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_sales</th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>date_activ</th>\n",
       "      <th>date_end</th>\n",
       "      <th>date_modif_prod</th>\n",
       "      <th>date_renewal</th>\n",
       "      <th>forecast_cons_12m</th>\n",
       "      <th>...</th>\n",
       "      <th>margin_gross_pow_ele</th>\n",
       "      <th>margin_net_pow_ele</th>\n",
       "      <th>nb_prod_act</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>num_years_antig</th>\n",
       "      <th>origin_up</th>\n",
       "      <th>pow_max</th>\n",
       "      <th>churn</th>\n",
       "      <th>offpeak_diff_dec_january_energy</th>\n",
       "      <th>offpeak_diff_dec_january_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>0</td>\n",
       "      <td>54946</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>25.44</td>\n",
       "      <td>25.44</td>\n",
       "      <td>2</td>\n",
       "      <td>678.99</td>\n",
       "      <td>3</td>\n",
       "      <td>lxidpiddsbxsbosboudacockeimpuepw</td>\n",
       "      <td>43.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>3.700961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>189.95</td>\n",
       "      <td>...</td>\n",
       "      <td>16.38</td>\n",
       "      <td>16.38</td>\n",
       "      <td>1</td>\n",
       "      <td>18.89</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.800</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003767</td>\n",
       "      <td>0.177779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>47.96</td>\n",
       "      <td>...</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.60</td>\n",
       "      <td>1</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.856</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>0.177779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bba03439a292a1e166f80264c16191cb</td>\n",
       "      <td>lmkebamcaaclubfxadlmueccxoimlema</td>\n",
       "      <td>1584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>240.04</td>\n",
       "      <td>...</td>\n",
       "      <td>30.22</td>\n",
       "      <td>30.22</td>\n",
       "      <td>1</td>\n",
       "      <td>25.46</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.200</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004547</td>\n",
       "      <td>0.177779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149d57cf92fc41cf94415803a877cb4b</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4425</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>445.75</td>\n",
       "      <td>...</td>\n",
       "      <td>44.91</td>\n",
       "      <td>44.91</td>\n",
       "      <td>1</td>\n",
       "      <td>47.98</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.006192</td>\n",
       "      <td>0.162916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                     channel_sales  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "3  bba03439a292a1e166f80264c16191cb  lmkebamcaaclubfxadlmueccxoimlema   \n",
       "4  149d57cf92fc41cf94415803a877cb4b                           MISSING   \n",
       "\n",
       "   cons_12m  cons_gas_12m  cons_last_month  date_activ    date_end  \\\n",
       "0         0         54946                0  2013-06-15  2016-06-15   \n",
       "1      4660             0                0  2009-08-21  2016-08-30   \n",
       "2       544             0                0  2010-04-16  2016-04-16   \n",
       "3      1584             0                0  2010-03-30  2016-03-30   \n",
       "4      4425             0              526  2010-01-13  2016-03-07   \n",
       "\n",
       "  date_modif_prod date_renewal  forecast_cons_12m  ...  margin_gross_pow_ele  \\\n",
       "0      2015-11-01   2015-06-23               0.00  ...                 25.44   \n",
       "1      2009-08-21   2015-08-31             189.95  ...                 16.38   \n",
       "2      2010-04-16   2015-04-17              47.96  ...                 28.60   \n",
       "3      2010-03-30   2015-03-31             240.04  ...                 30.22   \n",
       "4      2010-01-13   2015-03-09             445.75  ...                 44.91   \n",
       "\n",
       "   margin_net_pow_ele  nb_prod_act  net_margin  num_years_antig  \\\n",
       "0               25.44            2      678.99                3   \n",
       "1               16.38            1       18.89                6   \n",
       "2               28.60            1        6.60                6   \n",
       "3               30.22            1       25.46                6   \n",
       "4               44.91            1       47.98                6   \n",
       "\n",
       "                          origin_up pow_max  churn  \\\n",
       "0  lxidpiddsbxsbosboudacockeimpuepw  43.648      1   \n",
       "1  kamkkxfxxuwbdslkwifmmcsiusiuosws  13.800      0   \n",
       "2  kamkkxfxxuwbdslkwifmmcsiusiuosws  13.856      0   \n",
       "3  kamkkxfxxuwbdslkwifmmcsiusiuosws  13.200      0   \n",
       "4  kamkkxfxxuwbdslkwifmmcsiusiuosws  19.800      0   \n",
       "\n",
       "   offpeak_diff_dec_january_energy  offpeak_diff_dec_january_power  \n",
       "0                         0.020057                        3.700961  \n",
       "1                        -0.003767                        0.177779  \n",
       "2                        -0.004670                        0.177779  \n",
       "3                        -0.004547                        0.177779  \n",
       "4                        -0.006192                        0.162916  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the cleaned data and pricing data\n",
    "client_df = pd.read_csv(r'https://raw.githubusercontent.com/eeeeeedy/BGC/main/Task%202/client_data.csv')\n",
    "price_df = pd.read_csv(r'https://raw.githubusercontent.com/eeeeeedy/BGC/main/Task%202/price_data.csv')\n",
    "\n",
    "# Step 2: Recreate the feature\n",
    "# Group off-peak prices by companies and month\n",
    "monthly_price_by_id = price_df.groupby(['id', 'price_date']).agg({'price_off_peak_var': 'mean', 'price_off_peak_fix': 'mean'}).reset_index()\n",
    "\n",
    "# Get January and December prices\n",
    "jan_prices = monthly_price_by_id.groupby('id').first().reset_index()\n",
    "dec_prices = monthly_price_by_id.groupby('id').last().reset_index()\n",
    "\n",
    "# Calculate the difference\n",
    "diff = pd.merge(dec_prices.rename(columns={'price_off_peak_var': 'dec_1', 'price_off_peak_fix': 'dec_2'}), jan_prices.drop(columns='price_date'), on='id')\n",
    "diff['offpeak_diff_dec_january_energy'] = diff['dec_1'] - diff['price_off_peak_var']\n",
    "diff['offpeak_diff_dec_january_power'] = diff['dec_2'] - diff['price_off_peak_fix']\n",
    "diff = diff[['id', 'offpeak_diff_dec_january_energy', 'offpeak_diff_dec_january_power']]\n",
    "\n",
    "# Step 3: Merge this feature with the client data\n",
    "client_with_new_feature = pd.merge(client_df, diff, on='id', how='left')\n",
    "\n",
    "# Display the first few rows of the new dataframe\n",
    "client_with_new_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'channel_sales', 'cons_12m', 'cons_gas_12m', 'cons_last_month',\n",
       "       'date_activ', 'date_end', 'date_modif_prod', 'date_renewal',\n",
       "       'forecast_cons_12m', 'forecast_cons_year', 'forecast_discount_energy',\n",
       "       'forecast_meter_rent_12m', 'forecast_price_energy_off_peak',\n",
       "       'forecast_price_energy_peak', 'forecast_price_pow_off_peak', 'has_gas',\n",
       "       'imp_cons', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'nb_prod_act',\n",
       "       'net_margin', 'num_years_antig', 'origin_up', 'pow_max', 'churn',\n",
       "       'offpeak_diff_dec_january_energy', 'offpeak_diff_dec_january_power'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_with_new_feature.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the Feature’s Predictive Power\n",
    "\n",
    "To improve this feature's predictive power, let's consider the following:\n",
    "\n",
    "1. **Normalization**: Given that the consumption levels might vary significantly across customers, it might be beneficial to normalize this feature by the customer’s average monthly consumption.\n",
    "2. **Temporal Patterns**: We can also look at how this feature changes over time. For example, the average of the differences over the past three years might be a good indicator.\n",
    "\n",
    "Let's proceed to implement these improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_sales</th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>date_activ</th>\n",
       "      <th>date_end</th>\n",
       "      <th>date_modif_prod</th>\n",
       "      <th>date_renewal</th>\n",
       "      <th>forecast_cons_12m</th>\n",
       "      <th>...</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>num_years_antig</th>\n",
       "      <th>origin_up</th>\n",
       "      <th>pow_max</th>\n",
       "      <th>churn</th>\n",
       "      <th>offpeak_diff_dec_january_energy</th>\n",
       "      <th>offpeak_diff_dec_january_power</th>\n",
       "      <th>avg_monthly_cons</th>\n",
       "      <th>normalized_diff_energy</th>\n",
       "      <th>normalized_diff_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>0</td>\n",
       "      <td>54946</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>678.99</td>\n",
       "      <td>3</td>\n",
       "      <td>lxidpiddsbxsbosboudacockeimpuepw</td>\n",
       "      <td>43.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>3.700961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>189.95</td>\n",
       "      <td>...</td>\n",
       "      <td>18.89</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.800</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003767</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>388.333333</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>47.96</td>\n",
       "      <td>...</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.856</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>0.003922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bba03439a292a1e166f80264c16191cb</td>\n",
       "      <td>lmkebamcaaclubfxadlmueccxoimlema</td>\n",
       "      <td>1584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>240.04</td>\n",
       "      <td>...</td>\n",
       "      <td>25.46</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.200</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004547</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.001347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149d57cf92fc41cf94415803a877cb4b</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4425</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>445.75</td>\n",
       "      <td>...</td>\n",
       "      <td>47.98</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.006192</td>\n",
       "      <td>0.162916</td>\n",
       "      <td>368.750000</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                     channel_sales  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "3  bba03439a292a1e166f80264c16191cb  lmkebamcaaclubfxadlmueccxoimlema   \n",
       "4  149d57cf92fc41cf94415803a877cb4b                           MISSING   \n",
       "\n",
       "   cons_12m  cons_gas_12m  cons_last_month  date_activ    date_end  \\\n",
       "0         0         54946                0  2013-06-15  2016-06-15   \n",
       "1      4660             0                0  2009-08-21  2016-08-30   \n",
       "2       544             0                0  2010-04-16  2016-04-16   \n",
       "3      1584             0                0  2010-03-30  2016-03-30   \n",
       "4      4425             0              526  2010-01-13  2016-03-07   \n",
       "\n",
       "  date_modif_prod date_renewal  forecast_cons_12m  ...  net_margin  \\\n",
       "0      2015-11-01   2015-06-23               0.00  ...      678.99   \n",
       "1      2009-08-21   2015-08-31             189.95  ...       18.89   \n",
       "2      2010-04-16   2015-04-17              47.96  ...        6.60   \n",
       "3      2010-03-30   2015-03-31             240.04  ...       25.46   \n",
       "4      2010-01-13   2015-03-09             445.75  ...       47.98   \n",
       "\n",
       "   num_years_antig                         origin_up  pow_max  churn  \\\n",
       "0                3  lxidpiddsbxsbosboudacockeimpuepw   43.648      1   \n",
       "1                6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.800      0   \n",
       "2                6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.856      0   \n",
       "3                6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.200      0   \n",
       "4                6  kamkkxfxxuwbdslkwifmmcsiusiuosws   19.800      0   \n",
       "\n",
       "   offpeak_diff_dec_january_energy offpeak_diff_dec_january_power  \\\n",
       "0                         0.020057                       3.700961   \n",
       "1                        -0.003767                       0.177779   \n",
       "2                        -0.004670                       0.177779   \n",
       "3                        -0.004547                       0.177779   \n",
       "4                        -0.006192                       0.162916   \n",
       "\n",
       "   avg_monthly_cons  normalized_diff_energy  normalized_diff_power  \n",
       "0          0.000000                0.000000               0.000000  \n",
       "1        388.333333               -0.000010               0.000458  \n",
       "2         45.333333               -0.000103               0.003922  \n",
       "3        132.000000               -0.000034               0.001347  \n",
       "4        368.750000               -0.000017               0.000442  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average monthly consumption based on the 12-month consumption data ('cons_12m')\n",
    "client_with_new_feature['avg_monthly_cons'] = client_with_new_feature['cons_12m'] / 12\n",
    "\n",
    "# Now, proceed with the normalization using this new 'avg_monthly_cons' column\n",
    "client_with_new_feature['normalized_diff_energy'] = client_with_new_feature['offpeak_diff_dec_january_energy'] / client_with_new_feature['avg_monthly_cons']\n",
    "client_with_new_feature['normalized_diff_power'] = client_with_new_feature['offpeak_diff_dec_january_power'] / client_with_new_feature['avg_monthly_cons']\n",
    "\n",
    "# Handle infinities and NaNs generated due to division by zero in normalization\n",
    "client_with_new_feature.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "client_with_new_feature.fillna(0, inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "client_with_new_feature.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature has been improved with the following considerations:\n",
    "\n",
    "1. **Normalization**: We've normalized the difference between off-peak prices by the average monthly consumption for each customer. This helps account for varying consumption levels.\n",
    "2. **Handling Missing Values**: Any NaNs or infinities arising from the normalization process have been replaced with zeros.\n",
    "\n",
    "Now that we have an improved version of the existing feature, let's proceed to Sub-Task 2, where we'll build a predictive model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-Task 2: Build a Predictive Model\n",
    "\n",
    "#### Steps:\n",
    "1. Train a Random Forest classifier to predict customer churn.\n",
    "2. Evaluate the model's performance using suitable evaluation metrics.\n",
    "\n",
    "Let's start by training the Random Forest classifier. We'll use the following features:\n",
    "\n",
    "- The newly created and improved features\n",
    "- Features with relatively higher correlation with churn, as identified in the exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target variable\n",
    "features = ['normalized_diff_energy', 'normalized_diff_power', 'margin_net_pow_ele', 'margin_gross_pow_ele', 'num_years_antig']\n",
    "target = 'churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected features—`normalized_diff_energy`, `normalized_diff_power`, `margin_net_pow_ele`, `margin_gross_pow_ele`, and `num_years_antig`—were chosen based on a combination of factors:\n",
    "\n",
    "1. **Correlation with Target**: These features were among those that had higher correlations with the target variable 'churn' during the exploratory data analysis. A higher correlation usually indicates that the feature has a strong relationship with the target variable.\n",
    "\n",
    "2. **Business Understanding**: Features like `margin_net_pow_ele` and `margin_gross_pow_ele` could be directly related to customer satisfaction and, therefore, churn. Similarly, the number of years a customer has been with the company (`num_years_antig`) could be an indicator of loyalty and lower likelihood to churn.\n",
    "\n",
    "3. **Feature Engineering**: The features `normalized_diff_energy` and `normalized_diff_power` were engineered to capture the price sensitivity of the customer, which was the hypothesis we were testing.\n",
    "\n",
    "4. **Simplicity**: For the initial model, a smaller set of features was chosen to make the model less complex and easier to interpret.\n",
    "\n",
    "However, feature selection is an iterative process. Additional features could be included based on model performance and business requirements. Would you like to explore adding more features to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Confusion Matrix:\n",
      "[[2567   50]\n",
      " [ 288   17]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      2617\n",
      "           1       0.25      0.06      0.09       305\n",
      "\n",
      "    accuracy                           0.88      2922\n",
      "   macro avg       0.58      0.52      0.51      2922\n",
      "weighted avg       0.83      0.88      0.85      2922\n",
      "\n",
      "ROC AUC Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = client_with_new_feature[features]\n",
    "y = client_with_new_feature[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Task 2: Model Evaluation\n",
    "\n",
    "#### Model Performance Metrics:\n",
    "\n",
    "- **Accuracy**: Approximately 88.4%, which might seem good at first glance.\n",
    "- **Confusion Matrix**: Shows that the model is good at predicting the non-churn cases but not so good with the churn cases (True Positives are low).\n",
    "- **Classification Report**: The recall for churn (class 1) is very low at 6%, indicating the model is not good at identifying churn cases.\n",
    "- **ROC AUC Score**: Approximately 0.52, which is barely above a random classifier.\n",
    "\n",
    "#### Why These Metrics?\n",
    "\n",
    "- **Accuracy**: Gives a general idea of how many predictions are correct.\n",
    "- **Confusion Matrix**: Provides a breakdown of the true positive, true negative, false positive, and false negative rates.\n",
    "- **Classification Report**: Gives detailed metrics like precision, recall, and F1-score for each class.\n",
    "- **ROC AUC Score**: Useful for imbalanced classes and gives an idea of the trade-off between sensitivity and specificity.\n",
    "\n",
    "#### Advantages and Disadvantages of Random Forest\n",
    "\n",
    "- **Advantages**:\n",
    "    1. Handles both categorical and numerical features well.\n",
    "    2. Provides feature importance scores.\n",
    "    3. Generally robust to overfitting.\n",
    "  \n",
    "- **Disadvantages**:\n",
    "    1. Complexity: Random Forests can be quite complex to visualize or explain.\n",
    "    2. Computational Cost: Training can be time-consuming and computationally expensive.\n",
    "    3. May not perform well on imbalanced datasets, as seen here.\n",
    "\n",
    "#### Model Performance Satisfaction:\n",
    "\n",
    "The model's performance is not satisfactory due to the low recall for the churn class and a low ROC AUC score. It is essential to improve the model, especially in identifying customers who are likely to churn.\n",
    "\n",
    "#### Bonus: Financial Implication\n",
    "\n",
    "Offering a 20% discount to customers predicted to churn could be an effective strategy if the model were good at identifying such customers. However, given the current model's performance, this strategy might not be effective.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's consider some ways to improve the model's performance:\n",
    "\n",
    "1. **Class Balancing**: The current dataset is imbalanced. Methods like oversampling the minority class or using different evaluation metrics that are more robust to imbalance can be tried.\n",
    "  \n",
    "2. **Feature Engineering**: Additional features or transformations might improve the model.\n",
    "  \n",
    "3. **Parameter Tuning**: Hyperparameter tuning for the Random Forest model can also yield better results.\n",
    "\n",
    "Let's start with class balancing by oversampling the minority class. After that, we'll evaluate the model again to see if there's any improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7875\n",
      "Confusion Matrix:\n",
      "[[2212  405]\n",
      " [ 216   89]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      2617\n",
      "           1       0.18      0.29      0.22       305\n",
      "\n",
      "    accuracy                           0.79      2922\n",
      "   macro avg       0.55      0.57      0.55      2922\n",
      "weighted avg       0.83      0.79      0.81      2922\n",
      "\n",
      "ROC AUC: 0.5685\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Retrain the Random Forest Classifier with the balanced data\n",
    "rf_clf_smote = RandomForestClassifier(random_state=42)\n",
    "rf_clf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_smote = rf_clf_smote.predict(X_test)\n",
    "\n",
    "# Evaluate the model with the balanced data\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "conf_matrix_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "class_report_smote = classification_report(y_test, y_pred_smote)\n",
    "roc_auc_smote = roc_auc_score(y_test, y_pred_smote)\n",
    "\n",
    "# Use f-strings to format the output\n",
    "result_str = f\"Accuracy: {accuracy_smote:.4f}\\nConfusion Matrix:\\n{conf_matrix_smote}\\nClassification Report:\\n{class_report_smote}\\nROC AUC: {roc_auc_smote:.4f}\"\n",
    "\n",
    "print(result_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Comparison: Before and After Using SMOTE for Class Balancing\n",
    "\n",
    "#### Model After SMOTE:\n",
    "\n",
    "- **Accuracy**: Approximately 78.8%\n",
    "- **Confusion Matrix**: 2212 True Negatives, 405 False Positives, 216 False Negatives, 89 True Positives.\n",
    "- **Classification Report**: Recall for churn (class 1) improved to 29%.\n",
    "- **ROC AUC Score**: 0.5685\n",
    "\n",
    "#### Observations:\n",
    "\n",
    "1. **Accuracy**: The model's accuracy decreased from 88.4% to 78.8% after applying SMOTE. While this might seem like a deterioration, it's an acceptable trade-off since the model is now better at identifying churn cases.\n",
    "\n",
    "2. **Recall**: The recall for the churn class improved significantly, from 6% to 29%. This is a crucial improvement, as it indicates that the model is becoming better at identifying the minority class, i.e., customers who will churn.\n",
    "\n",
    "3. **ROC AUC Score**: A minor improvement in the ROC AUC score from 0.52 to 0.5685 indicates that the model's ability to distinguish between the churn and non-churn cases has improved slightly.\n",
    "\n",
    "4. **Precision and F1-Score**: The precision for identifying churn is still low, and so is the F1-score. This suggests that while the model is identifying more churn cases, it's also misclassifying non-churn cases as churn (False Positives).\n",
    "\n",
    "Overall, the application of SMOTE has made the model more sensitive to the churn class, although at the expense of some accuracy. The model is now better aligned with the business objective of identifying customers who are more likely to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Tuning:\n",
    "\n",
    "Exploring additional hyperparameter tuning could potentially improve your model's performance. Here are some strategies:\n",
    "\n",
    "   **Random Search: Explore the hyperparameter space to find the best set of hyperparameters for the Random Forest model.**\n",
    "\n",
    "Let's begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Accuracy: 0.79\n",
      "Confusion Matrix:\n",
      "[[2226  391]\n",
      " [ 222   83]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      2617\n",
      "           1       0.18      0.27      0.21       305\n",
      "\n",
      "    accuracy                           0.79      2922\n",
      "   macro avg       0.54      0.56      0.55      2922\n",
      "weighted avg       0.83      0.79      0.81      2922\n",
      "\n",
      "ROC AUC Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for Random Forest\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest Classifier\n",
    "rf_clf_for_tuning = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Random Search\n",
    "random_search = RandomizedSearchCV(estimator=rf_clf_for_tuning, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Perform Random Search on balanced data\n",
    "random_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best parameters and estimator\n",
    "best_params = random_search.best_params_\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best estimator\n",
    "y_pred_best = best_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "conf_matrix_best = confusion_matrix(y_test, y_pred_best)\n",
    "class_report_best = classification_report(y_test, y_pred_best)\n",
    "roc_auc_best = roc_auc_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_best:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix_best}\")\n",
    "print(f\"Classification Report:\\n{class_report_best}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_best:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Model Evaluation Comparison: Before and After Hyperparameter Tuning\n",
    "\n",
    "#### Metrics for Each Model:\n",
    "\n",
    "- **Random Forest without SMOTE**: \n",
    "    - Accuracy: ~88.4%, ROC AUC: 0.52\n",
    "- **Random Forest with SMOTE**: \n",
    "    - Accuracy: ~78.8%, ROC AUC: 0.5685\n",
    "- **Random Forest with SMOTE & Hyperparameter Tuning**: \n",
    "    - Accuracy: ~79%, ROC AUC: 0.56\n",
    "\n",
    "#### Explanation of Model Performance:\n",
    "\n",
    "The model primarily underperformed in the following areas:\n",
    "\n",
    "1. **Low Precision for Churn**: Across all variations of the model, precision for identifying churn remained low, hovering around 18%. This indicates that the model is flagging a large number of false positives, i.e., predicting that customers will churn when they actually will not. This could lead to unnecessary costs if these customers are targeted with retention efforts.\n",
    "\n",
    "2. **Moderate Recall after SMOTE and Tuning**: Although the recall for the churn class improved with SMOTE and hyperparameter tuning, it reached only about 27-29%. This means that the model is still missing a significant number of customers who end up churning. This could lead to lost revenue and increased customer acquisition costs.\n",
    "\n",
    "3. **ROC AUC Score**: The ROC AUC score remained moderate (0.56) even after hyperparameter tuning. This metric indicates the model's ability to distinguish between the churn and non-churn classes, and an ideal score would be close to 1. \n",
    "\n",
    "4. **Trade-Off in Accuracy**: The accuracy of the model decreased after using SMOTE, from approximately 88.4% to around 79%. While this was expected due to the balancing of classes, it's a trade-off that may not be acceptable in all business contexts.\n",
    "\n",
    "Each of these underperformances has implications for customer retention and the cost effectiveness of any retention campaigns the business may undertake.\n",
    "#### Advantages and Disadvantages of Random Forest:\n",
    "\n",
    "- **Advantages**:\n",
    "    1. Handles imbalanced classes reasonably well.\n",
    "    2. Good for both categorical and numerical features.\n",
    "  \n",
    "- **Disadvantages**:\n",
    "    1. Complex and not easily interpretable.\n",
    "    2. Computationally intensive for hyperparameter tuning.\n",
    "\n",
    "#### Model Performance Satisfaction:\n",
    "\n",
    "The model's performance is not entirely satisfactory. Although recall improved with SMOTE and hyperparameter tuning, precision remained low. The model still struggles with identifying churn accurately, leading to a high number of false positives.\n",
    "\n",
    "#### Bonus: Financial Implication\n",
    "\n",
    "**Assumptions**:\n",
    "1. Cost to retain a customer through discounts is 70% less than acquiring a new one.\n",
    "2. False positives will also receive the discount, incurring a cost.\n",
    "\n",
    "Using the model, especially the one after hyperparameter tuning, the client can target a larger subset of customers who are more likely to churn and offer them discounts to retain them. However, the low precision means we'll also be offering discounts to customers who wouldn't have churned, thereby incurring some loss. A more balanced recall and precision would make the model more financially beneficial for the client."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
